package protocol;

message FieldValue {
  optional string string_value = 1;
  optional double double_value = 3;
  optional bool bool_value = 4;
  optional int64 int64_value = 5;
}

message Point {
  repeated FieldValue values = 1;
  optional int64 timestamp = 2;
  optional uint64 sequence_number = 3;
}

message Series {
  repeated Point points = 1;
  required string name = 2;
  repeated string fields = 3;
}

message QueryResponseChunk {
  optional Series series = 1;
  optional bool done = 2;
}

message Request {
  enum Type {
    QUERY = 1;
    REPLICATION_WRITE = 2;
    PROXY_WRITE = 3;
    REPLICATION_DELETE = 4;
    PROXY_DELETE = 5;
    REPLICATION_REPLAY = 6;
    LIST_SERIES = 7;
    SEQUENCE_NUMBER = 8;
    PROXY_DROP_DATABASE = 9;
    REPLICATION_DROP_DATABASE = 10;
    PROXY_DROP_SERIES = 11;
    REPLICATION_DROP_SERIES = 12;
  }
  required uint32 id = 1;
  required Type type = 2;
  required string database = 3;
  optional Series series = 4;
  // only write and delete requests get sequenceNumbers assigned. These are used to
  // ensure that the receiving server is up to date
  optional uint64 sequence_number = 5;
  // the originzatingServerId is only used for writes and deletes. It is the id of the
  // server that first committed the write to its local datastore. It is used for
  // the other servers in the hash ring to ensure they remain consistent.
  optional uint32 originating_server_id = 6;
  optional uint32 cluster_version = 10;
  optional string query = 7;
  optional string user_name = 8;
  // ringLocationsToQuery tells the server what data it should be returning.
  // for example, if the number is 1, it will only return data that is owned by 
  // this server on the hash ring. If 2, it will return this server and data replicated
  // from the server directly before it on the ring. 3, etc.
  // If this field is left out, we assume that we'll be returning all data the server has
  // for the query.
  optional uint32 ring_locations_to_query = 9;
  // optional fields for replication replay requests. should include originating serer id
  optional uint32 replication_factor = 16;
  optional uint32 owner_server_id = 17;
  optional uint64 last_known_sequence_number = 18;
}

message Response {
  enum Type {
    QUERY = 1;
    WRITE_OK = 2;
    END_STREAM = 3;
    REPLICATION_REPLAY = 4;
    REPLICATION_REPLAY_END = 5;
    LIST_SERIES = 6;
    SEQUENCE_NUMBER = 7;
  }
  enum ErrorCode {
    REQUEST_TOO_LARGE = 1;
    INTERNAL_ERROR = 2;
  }
  required Type type = 1;
  required uint32 request_id = 2;
  optional Series series = 3;
  optional ErrorCode error_code = 4;
  optional string error_message = 5;
  optional Request request = 7;
  optional QueryMapResult map_result = 6;
}

// The Map phase of a query outputs map results. These are
// then shuffled together and reduced. Since all shuffles are
// based on time, the end_time, start_time, and next_time let
// the ReducePhase of the job know how to shuffle them together
// even when the MapResults come in through multiple streams (like
// multiple servers)
message MapResult {
  repeated MapGroupResult mapped_groups = 1;
  required string series = 2;
  required int64 end_time = 3;
  required int64 start_time = 4;
  optional int64 next_time = 5;
}

// Map results come for each group in a query. For example
// if you have select count(foo) from bar group by org_id.
// Each unique org id would have a map group result associated 
// with it. Or if you had 'group by time(5m), org_id'
// each unique org id per five minute interval would have a 
// map group result. Each five minute window would thus have
// an associated MapResult from each server (assuming there was
// data in that window of time)
message MapGroupResult {
  required bytes data = 1;
  required bytes group = 2;
  enum Reducer {
    POINTS = 1;
    SUM = 2;
    COUNT = 3;
    MIN = 4;
    MAX = 5;
    MEAN = 6;
    FIRST = 7;
    LAST = 8;
    CUSTOM = 9;
  }
}
